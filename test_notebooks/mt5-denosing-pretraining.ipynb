{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:15:42.635396Z","iopub.execute_input":"2025-08-31T09:15:42.635914Z","iopub.status.idle":"2025-08-31T09:15:42.958210Z","shell.execute_reply.started":"2025-08-31T09:15:42.635881Z","shell.execute_reply":"2025-08-31T09:15:42.957421Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Pretrain mT5 with span-corruption denoising on OPUS-100 (Hugging Face dataset).","metadata":{}},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:15:42.959293Z","iopub.execute_input":"2025-08-31T09:15:42.959601Z","iopub.status.idle":"2025-08-31T09:15:49.295988Z","shell.execute_reply.started":"2025-08-31T09:15:42.959577Z","shell.execute_reply":"2025-08-31T09:15:49.295336Z"}},"outputs":[{"name":"stdout","text":"4.52.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import logging\nimport random\nfrom typing import List, Dict\n\nimport numpy as np\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\nimport torch\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:15:49.296678Z","iopub.execute_input":"2025-08-31T09:15:49.296983Z","iopub.status.idle":"2025-08-31T09:16:12.009246Z","shell.execute_reply.started":"2025-08-31T09:15:49.296965Z","shell.execute_reply":"2025-08-31T09:16:12.008618Z"}},"outputs":[{"name":"stderr","text":"2025-08-31 09:15:58.581421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756631758.779292      76 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756631758.832848      76 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ---- Dataset processing ----\ndef flatten_parallel_to_monolingual(batch):\n    out_texts = []\n    translations = batch.get(\"translation\", {})\n    if isinstance(translations, dict):\n        for lang, text in translations.items():\n            if text:\n                txt = text.strip()\n                if len(txt) > 0:\n                    out_texts.append({\"text\": txt, \"lang\": lang})\n    elif isinstance(translations, list):\n        for t in translations:\n            if isinstance(t, dict):\n                for lang, text in t.items():\n                    if text:\n                        txt = text.strip()\n                        if len(txt) > 0:\n                            out_texts.append({\"text\": txt, \"lang\": lang})\n    return {\"__flattened\": out_texts}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:16:12.011230Z","iopub.execute_input":"2025-08-31T09:16:12.012148Z","iopub.status.idle":"2025-08-31T09:16:12.016858Z","shell.execute_reply.started":"2025-08-31T09:16:12.012120Z","shell.execute_reply":"2025-08-31T09:16:12.016129Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ---------- Config ----------\nMODEL_NAME = \"google/mt5-base\"\nHF_DATASET_NAME = \"Helsinki-NLP/opus-100\"\nOUTPUT_DIR = \"./mt5-opus100-denoise\"\nMAX_INPUT_LENGTH = 256\nMAX_TARGET_LENGTH = 128\nNOISE_DENSITY = 0.15\nMEAN_NOISE_SPAN_LENGTH = 3.0\nTRAIN_SAMPLE_SIZE = 5000  # set small int for debugging\n# ----------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:16:12.017621Z","iopub.execute_input":"2025-08-31T09:16:12.017870Z","iopub.status.idle":"2025-08-31T09:16:12.047342Z","shell.execute_reply.started":"2025-08-31T09:16:12.017846Z","shell.execute_reply":"2025-08-31T09:16:12.046550Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def prepare_dataset(split=\"train\"):\n    print(\"prepare datasets coming to method\")\n    logger.info(f\"Loading dataset {HF_DATASET_NAME} split={split} ...\")\n    ds = load_dataset(HF_DATASET_NAME,'en-si' ,split=split, streaming=False)\n\n    logger.info(\"Flattening parallel translations into monolingual sentences ...\")\n    ds = ds.map(flatten_parallel_to_monolingual, batched=False, remove_columns=ds.column_names)\n\n    texts = []\n    for item in ds:\n        for d in item.get(\"__flattened\", []):\n            texts.append(d)\n    mono = Dataset.from_list(texts)\n    logger.info(f\"Monolingual dataset size: {len(mono)}\")\n\n    if TRAIN_SAMPLE_SIZE:\n        mono = mono.shuffle(seed=42).select(range(min(TRAIN_SAMPLE_SIZE, len(mono))))\n        logger.info(f\"Using sample size: {len(mono)}\")\n    return mono","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:16:12.048169Z","iopub.execute_input":"2025-08-31T09:16:12.048399Z","iopub.status.idle":"2025-08-31T09:16:12.062836Z","shell.execute_reply.started":"2025-08-31T09:16:12.048381Z","shell.execute_reply":"2025-08-31T09:16:12.062072Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ---- Fixed T5 span corruption collator ----\nclass T5DenoisingCollator:\n    def __init__(self, tokenizer, noise_density=0.15, mean_noise_span_length=3.0,\n                 input_length=256, target_length=128):\n        self.tokenizer = tokenizer\n        self.noise_density = noise_density\n        self.mean_noise_span_length = mean_noise_span_length\n        self.input_length = input_length\n        self.target_length = target_length\n        self.pad_token_id = tokenizer.pad_token_id\n        self.vocab_size = tokenizer.vocab_size\n        self.sentinel_start = 0  # will map to <extra_id_0> later\n\n    def __call__(self, examples):\n        # Convert examples to token IDs\n        input_ids_list = [e[\"input_ids\"] for e in examples]\n        batch_input_ids = []\n        batch_labels = []\n\n        for ids in input_ids_list:\n            corrupted, labels = self._span_corrupt(ids)\n            batch_input_ids.append(corrupted)\n            batch_labels.append(labels)\n\n        batch = {\n            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n        }\n\n        # Attention mask\n        batch[\"attention_mask\"] = (batch[\"input_ids\"] != self.pad_token_id).long()\n\n        # --- DEBUG PRINTS ---\n        # print(\"Example input_ids:\", batch[\"input_ids\"][0][:20])\n        # print(\"Example labels   :\", batch[\"labels\"][0][:20])\n        # print(\"Masked tokens in labels:\", (batch[\"labels\"][0] != -100).sum().item())\n\n        return batch\n\n    def _span_corrupt(self, tokens: List[int]):\n        \"\"\"T5-style span corruption\"\"\"\n        tokens = [t for t in tokens if t != self.pad_token_id]\n        num_to_mask = max(1, int(len(tokens) * self.noise_density))\n\n        # Determine spans\n        span_starts = []\n        i = 0\n        while i < num_to_mask:\n            span_len = max(1, np.random.poisson(self.mean_noise_span_length))\n            start = random.randint(0, max(0, len(tokens) - span_len))\n            span_starts.append((start, min(len(tokens), start + span_len)))\n            i += span_len\n\n        # Sort spans\n        span_starts = sorted(span_starts, key=lambda x: x[0])\n\n        corrupted = []\n        labels = []\n        sentinel_id = self.tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n\n        last_idx = 0\n        current_sentinel = sentinel_id\n        for start, end in span_starts:\n            # Add unmasked tokens\n            corrupted.extend(tokens[last_idx:start])\n            # Add sentinel to input\n            corrupted.append(current_sentinel)\n            # Add sentinel + masked tokens to labels\n            labels.append(current_sentinel)\n            labels.extend(tokens[start:end])\n            current_sentinel += 1  # next sentinel token\n            last_idx = end\n\n        # Add remaining tokens\n        corrupted.extend(tokens[last_idx:])\n        labels.append(self.pad_token_id)\n\n        # Truncate / pad\n        corrupted = corrupted[:self.input_length] + [self.pad_token_id] * max(0, self.input_length - len(corrupted))\n        labels = labels[:self.target_length] + [-100] * max(0, self.target_length - len(labels))\n\n        return corrupted, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:16:12.063602Z","iopub.execute_input":"2025-08-31T09:16:12.063811Z","iopub.status.idle":"2025-08-31T09:16:12.080114Z","shell.execute_reply.started":"2025-08-31T09:16:12.063794Z","shell.execute_reply":"2025-08-31T09:16:12.079396Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:16:12.080817Z","iopub.execute_input":"2025-08-31T09:16:12.080988Z","iopub.status.idle":"2025-08-31T09:16:18.197879Z","shell.execute_reply.started":"2025-08-31T09:16:12.080973Z","shell.execute_reply":"2025-08-31T09:16:18.197099Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d4462183d9546b3abcf1203451c8ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f900378d060497cb3574714f5669c6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08a960e7f7341d4a5d43ea67aae0b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d230686b53564603b055be109fba60b9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:16:18.198735Z","iopub.execute_input":"2025-08-31T09:16:18.198967Z","iopub.status.idle":"2025-08-31T09:16:34.586770Z","shell.execute_reply.started":"2025-08-31T09:16:18.198951Z","shell.execute_reply":"2025-08-31T09:16:34.584773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94fe33992d34016a314b638d9343609"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db360b2926e4d0b83e23b974b5b3ff8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1c762a3bd74bec83e4b9214ff033fa"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"mono = prepare_dataset(\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:16:34.596996Z","iopub.execute_input":"2025-08-31T09:16:34.597398Z","iopub.status.idle":"2025-08-31T09:17:58.028422Z","shell.execute_reply.started":"2025-08-31T09:16:34.597365Z","shell.execute_reply":"2025-08-31T09:17:58.027695Z"}},"outputs":[{"name":"stdout","text":"prepare datasets coming to method\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fd22310241d433a8be70ce0fd7597c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/155k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb11eac4fc194ae1b8c01464a9456285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/65.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1ff9100efe746b39388ed547a1c1d22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/153k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f093c704a644a4a2234f6a1db0eb1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4eaa93d61a1464fa109a2deb16c3822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/979109 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68d1aa90bd0847cbaf547fe05129059d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70aada76a3ad4c9bb4f4e1e62d0224e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/979109 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d5e20881f744d297f5f9125ceb2df6"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"mono","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.029228Z","iopub.execute_input":"2025-08-31T09:17:58.029419Z","iopub.status.idle":"2025-08-31T09:17:58.034000Z","shell.execute_reply.started":"2025-08-31T09:17:58.029403Z","shell.execute_reply":"2025-08-31T09:17:58.033386Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['lang', 'text'],\n    num_rows: 5000\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"print(mono[:10]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.034860Z","iopub.execute_input":"2025-08-31T09:17:58.035061Z","iopub.status.idle":"2025-08-31T09:17:58.055987Z","shell.execute_reply.started":"2025-08-31T09:17:58.035044Z","shell.execute_reply":"2025-08-31T09:17:58.055393Z"}},"outputs":[{"name":"stdout","text":"{'lang': ['si', 'en', 'si', 'en', 'si', 'si', 'en', 'en', 'si', 'en'], 'text': ['ඉතින්...', \"Um... we'll have the next drink on the plane, okay?\", 'හරි ඔයා දැන් ඔහුට කැමති නැද්ද ?', 'He got her...', 'අලුතෙන්ම වකුගඩුවක් ගන්න එක ලොකු වැඩක් බන්.', 'අනේ දෙවියනේ!', 'That is the coolest name ever!', 'Have this.', 'මයිල්ස්!', 'Anjali?']}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"tokenized = mono.map(lambda ex: tokenizer(ex[\"text\"], truncation=True,\n                                              max_length=MAX_INPUT_LENGTH),\n                         batched=True, remove_columns=[\"text\", \"lang\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.056640Z","iopub.execute_input":"2025-08-31T09:17:58.056835Z","iopub.status.idle":"2025-08-31T09:17:58.674456Z","shell.execute_reply.started":"2025-08-31T09:17:58.056820Z","shell.execute_reply":"2025-08-31T09:17:58.673666Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3afba584d4ae464dab26780329e22902"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.675319Z","iopub.execute_input":"2025-08-31T09:17:58.675567Z","iopub.status.idle":"2025-08-31T09:17:58.680615Z","shell.execute_reply.started":"2025-08-31T09:17:58.675540Z","shell.execute_reply":"2025-08-31T09:17:58.679805Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 5000\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"print(tokenized[:10]) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.681251Z","iopub.execute_input":"2025-08-31T09:17:58.681497Z","iopub.status.idle":"2025-08-31T09:17:58.695823Z","shell.execute_reply.started":"2025-08-31T09:17:58.681474Z","shell.execute_reply":"2025-08-31T09:17:58.695302Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': [[3686, 35420, 302, 1], [3048, 302, 787, 277, 1578, 783, 287, 6844, 20561, 351, 287, 30438, 261, 259, 69381, 291, 1], [24277, 3165, 8965, 259, 29799, 3165, 6640, 1489, 16711, 1586, 6114, 17318, 38803, 259, 291, 1], [1669, 5666, 1001, 302, 1], [58255, 234438, 75328, 259, 80583, 163816, 91298, 1939, 26171, 4258, 42285, 22928, 18827, 1939, 259, 176281, 260, 1], [2022, 17635, 7106, 32832, 17635, 309, 1], [7961, 339, 287, 16223, 861, 6535, 14049, 309, 1], [21201, 714, 260, 1], [259, 38524, 4858, 3775, 309, 1], [298, 204413, 291, 1]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1]]}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"data_collator = T5DenoisingCollator(tokenizer, NOISE_DENSITY,\n                                        MEAN_NOISE_SPAN_LENGTH,\n                                        MAX_INPUT_LENGTH,\n                                        MAX_TARGET_LENGTH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.696538Z","iopub.execute_input":"2025-08-31T09:17:58.696731Z","iopub.status.idle":"2025-08-31T09:17:58.711144Z","shell.execute_reply.started":"2025-08-31T09:17:58.696714Z","shell.execute_reply":"2025-08-31T09:17:58.710476Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n        output_dir=OUTPUT_DIR,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=8,\n        fp16=True,\n        optim=\"adafactor\",\n        learning_rate=1e-4,\n        warmup_ratio=0.01,\n        num_train_epochs=1,\n        logging_steps=500,\n        save_steps=10000,\n        save_total_limit=3,\n        remove_unused_columns=False,\n        report_to=\"none\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.711918Z","iopub.execute_input":"2025-08-31T09:17:58.712160Z","iopub.status.idle":"2025-08-31T09:17:58.756673Z","shell.execute_reply.started":"2025-08-31T09:17:58.712139Z","shell.execute_reply":"2025-08-31T09:17:58.756114Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:58.757406Z","iopub.execute_input":"2025-08-31T09:17:58.757607Z","iopub.status.idle":"2025-08-31T09:17:59.784444Z","shell.execute_reply.started":"2025-08-31T09:17:58.757590Z","shell.execute_reply":"2025-08-31T09:17:59.783836Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_76/1713224962.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Start training ...\")\ntrainer.train()\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:17:59.785219Z","iopub.execute_input":"2025-08-31T09:17:59.785444Z","iopub.status.idle":"2025-08-31T09:25:13.993970Z","shell.execute_reply.started":"2025-08-31T09:17:59.785425Z","shell.execute_reply":"2025-08-31T09:25:13.992920Z"}},"outputs":[{"name":"stdout","text":"Start training ...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [157/157 07:03, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Done.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\nfrom huggingface_hub import notebook_login\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:26:50.413878Z","iopub.execute_input":"2025-08-31T09:26:50.414232Z","iopub.status.idle":"2025-08-31T09:26:50.424836Z","shell.execute_reply.started":"2025-08-31T09:26:50.414209Z","shell.execute_reply":"2025-08-31T09:26:50.424077Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:32:38.101468Z","iopub.execute_input":"2025-08-31T09:32:38.102001Z","iopub.status.idle":"2025-08-31T09:32:38.117925Z","shell.execute_reply.started":"2025-08-31T09:32:38.101977Z","shell.execute_reply":"2025-08-31T09:32:38.117010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e7039c3b984cc3a3f1544735a49fe2"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"\n# Step 2: Load your trained model + tokenizer (replace with your paths)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"./mt5-opus100-denoise\")\ntokenizer = AutoTokenizer.from_pretrained(\"./mt5-opus100-denoise\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:42:46.341143Z","iopub.execute_input":"2025-08-31T09:42:46.341424Z","iopub.status.idle":"2025-08-31T09:42:46.962572Z","shell.execute_reply.started":"2025-08-31T09:42:46.341406Z","shell.execute_reply":"2025-08-31T09:42:46.961999Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Step 3: Push to Hugging Face Hub\nrepo_name = \"Eshan210352R/mt5-opus100-denoise-EN-SI\"\n\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)\n\nprint(f\"✅ Model uploaded! Check it here: https://huggingface.co/{repo_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T09:44:13.981281Z","iopub.execute_input":"2025-08-31T09:44:13.981997Z","iopub.status.idle":"2025-08-31T09:45:24.374475Z","shell.execute_reply.started":"2025-08-31T09:44:13.981972Z","shell.execute_reply":"2025-08-31T09:45:24.373608Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ece9b1029f3b45c08dd04bd88bb706b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"371acbc80227400fa8608d61c64ce96a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937a458f72c24713932533a4f621b105"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01fd57b188c54a8c8259ebdcd3a335fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"257ca892c89c45028473323d5ed980a8"}},"metadata":{}},{"name":"stdout","text":"✅ Model uploaded! Check it here: https://huggingface.co/Eshan210352R/mt5-opus100-denoise-EN-SI\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Perplexity Metric Evaluation","metadata":{}},{"cell_type":"markdown","source":"#### What is Perplexity?\n\n- Perplexity (PPL) is a metric used to evaluate language models. It measures how well a probability model predicts a sample","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom datasets import load_dataset\nimport math\nimport random\nimport numpy as np\n\n# ---------------- Config ----------------\nMODEL_PATH = \"./mt5-opus100-denoise\"  # pretrained model\nHF_DATASET_NAME = \"Helsinki-NLP/opus-100\"\nSRC_LANG = \"en\"\nTGT_LANG = \"si\"\nMAX_INPUT_LENGTH = 256\nMAX_TARGET_LENGTH = 128\nBATCH_SIZE = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nNUM_SAMPLES = 10  # subset for demo\nNOISE_DENSITY = 0.15\nMEAN_NOISE_SPAN_LENGTH = 3.0\n# ----------------------------------------\n\n# --- Load dataset ---\nprint(\"Loading dataset...\")\nds = load_dataset(HF_DATASET_NAME, f\"{SRC_LANG}-{TGT_LANG}\", split=\"test\")\nds = ds.select(range(NUM_SAMPLES))\ntexts = [ex[\"translation\"][SRC_LANG] for ex in ds]\nprint(f\"Loaded {len(texts)} examples.\")\nprint(\"Sample:\", texts[:2])\n\n# --- Collator for T5 span corruption ---\nclass T5DenoisingCollator:\n    def __init__(self, tokenizer, noise_density=0.15, mean_noise_span_length=3.0,\n                 input_length=256, target_length=128):\n        self.tokenizer = tokenizer\n        self.noise_density = noise_density\n        self.mean_noise_span_length = mean_noise_span_length\n        self.input_length = input_length\n        self.target_length = target_length\n        self.pad_token_id = tokenizer.pad_token_id\n\n    def __call__(self, texts):\n        input_ids_list = []\n        labels_list = []\n        for text in texts:\n            enc = self.tokenizer(text, truncation=True, max_length=self.input_length)\n            corrupted, labels = self._span_corrupt(enc[\"input_ids\"])\n            input_ids_list.append(corrupted)\n            labels_list.append(labels)\n\n        batch = {\n            \"input_ids\": torch.tensor(input_ids_list, dtype=torch.long).to(DEVICE),\n            \"labels\": torch.tensor(labels_list, dtype=torch.long).to(DEVICE),\n        }\n        batch[\"attention_mask\"] = (batch[\"input_ids\"] != self.pad_token_id).long()\n        return batch\n\n    def _span_corrupt(self, tokens):\n        tokens = [t for t in tokens if t != self.pad_token_id]\n        num_to_mask = max(1, int(len(tokens) * self.noise_density))\n        spans = []\n        i = 0\n        while i < num_to_mask:\n            span_len = max(1, np.random.poisson(self.mean_noise_span_length))\n            start = random.randint(0, max(0, len(tokens)-span_len))\n            spans.append((start, min(len(tokens), start+span_len)))\n            i += span_len\n        spans = sorted(spans, key=lambda x: x[0])\n\n        corrupted = []\n        labels = []\n        sentinel_id = self.tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n        current_sentinel = sentinel_id\n        last_idx = 0\n        for start, end in spans:\n            corrupted.extend(tokens[last_idx:start])\n            corrupted.append(current_sentinel)\n            labels.append(current_sentinel)\n            labels.extend(tokens[start:end])\n            current_sentinel += 1\n            last_idx = end\n        corrupted.extend(tokens[last_idx:])\n        labels.append(self.pad_token_id)\n\n        # Pad/truncate\n        corrupted = corrupted[:self.input_length] + [self.pad_token_id] * max(0, self.input_length-len(corrupted))\n        labels = labels[:self.target_length] + [-100] * max(0, self.target_length-len(labels))\n        return corrupted, labels\n\n# --- Load model & tokenizer ---\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH).to(DEVICE)\nmodel.eval()\n\ncollator = T5DenoisingCollator(tokenizer, NOISE_DENSITY, MEAN_NOISE_SPAN_LENGTH,\n                               MAX_INPUT_LENGTH, MAX_TARGET_LENGTH)\n\n# --- Evaluate PPL and print sample predictions ---\ndef evaluate(model, tokenizer, texts, collator, batch_size=4, max_samples_print=5):\n    nlls = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        batch = collator(batch_texts)\n        with torch.no_grad():\n            outputs = model(**batch)  # labels already in batch\n            nlls.append(outputs.loss.item())\n\n            # Print predictions for first batch\n            if i == 0:\n                preds = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n                                       max_length=MAX_TARGET_LENGTH)\n                decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n                decoded_labels = tokenizer.batch_decode([[t for t in l if t != -100] for l in batch[\"labels\"]],\n                                                        skip_special_tokens=True)\n                print(\"\\n--- Sample Reconstructions ---\")\n                for src, lab, pred in zip(batch_texts[:max_samples_print],\n                                          decoded_labels[:max_samples_print],\n                                          decoded_preds[:max_samples_print]):\n                    print(f\"Source   : {src}\")\n                    print(f\"Target   : {lab}\")\n                    print(f\"Predicted: {pred}\")\n                    print(\"-----------\")\n\n    avg_nll = sum(nlls)/len(nlls)\n    ppl = math.exp(avg_nll)\n    return ppl\n\nprint(\"\\nEvaluating pretrained model...\")\nppl = evaluate(model, tokenizer, texts, collator, batch_size=BATCH_SIZE)\nprint(f\"\\nPerplexity: {ppl:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T10:44:47.872452Z","iopub.execute_input":"2025-08-31T10:44:47.872710Z","iopub.status.idle":"2025-08-31T10:44:51.774437Z","shell.execute_reply.started":"2025-08-31T10:44:47.872693Z","shell.execute_reply":"2025-08-31T10:44:51.773601Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nLoaded 10 examples.\nSample: ['Because I believed in destiny and fate..', 'You will get know slowly.']\n\nEvaluating pretrained model...\n\n--- Sample Reconstructions ---\nSource   : Because I believed in destiny and fate..\nTarget   : d in\nPredicted: <extra_id_0>..\n-----------\nSource   : You will get know slowly.\nTarget   : know slowly\nPredicted: <extra_id_0>.\n-----------\nSource   : Buckle up, guys. I am increasing the speed.\nTarget   : , <0x00> \nPredicted: <extra_id_0><0x00>s.\n-----------\nSource   : I'm going to the bathroom again.\nTarget   : m going\nPredicted: <extra_id_0> again.\n-----------\n\nPerplexity: 45727738347.61\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}