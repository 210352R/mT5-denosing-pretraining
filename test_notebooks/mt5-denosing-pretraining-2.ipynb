{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:43:41.310745Z","iopub.execute_input":"2025-09-01T04:43:41.311004Z","iopub.status.idle":"2025-09-01T04:43:41.565382Z","shell.execute_reply.started":"2025-09-01T04:43:41.310980Z","shell.execute_reply":"2025-09-01T04:43:41.564581Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"! pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:43:41.566926Z","iopub.execute_input":"2025-09-01T04:43:41.567423Z","iopub.status.idle":"2025-09-01T04:43:46.180749Z","shell.execute_reply.started":"2025-09-01T04:43:41.567384Z","shell.execute_reply":"2025-09-01T04:43:46.179826Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.5.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import logging\nimport random\nfrom typing import List, Dict\nimport numpy as np\nfrom datasets import load_dataset, Dataset, concatenate_datasets\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n    pipeline\n)\nimport torch\nfrom tqdm import tqdm\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:43:46.181844Z","iopub.execute_input":"2025-09-01T04:43:46.182142Z","iopub.status.idle":"2025-09-01T04:44:12.428715Z","shell.execute_reply.started":"2025-09-01T04:43:46.182109Z","shell.execute_reply":"2025-09-01T04:44:12.427863Z"}},"outputs":[{"name":"stderr","text":"2025-09-01 04:44:00.108387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756701840.270886      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756701840.319640      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# ---------- Config ----------\nMODEL_NAME = \"google/mt5-base\"\nHF_DATASET_NAME = \"Helsinki-NLP/opus-100\"\nOUTPUT_DIR = \"./mt5-opus100-denoise-final\"\n # Change this\n\nMAX_INPUT_LENGTH = 256\nMAX_TARGET_LENGTH = 128\nNOISE_DENSITY = 0.15\nMEAN_NOISE_SPAN_LENGTH = 3.0\nTRAIN_SAMPLE_SIZE = 1000 # Increased for better training\nVAL_SAMPLE_SIZE = 250\nLANGUAGE_PAIRS = ['en-si']  # Multiple languages\n# ----------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:44:12.430995Z","iopub.execute_input":"2025-09-01T04:44:12.431674Z","iopub.status.idle":"2025-09-01T04:44:12.437095Z","shell.execute_reply.started":"2025-09-01T04:44:12.431639Z","shell.execute_reply":"2025-09-01T04:44:12.436295Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def prepare_datasets():\n    \"\"\"Prepare training and validation datasets from multiple language pairs\"\"\"\n    train_datasets = []\n    val_datasets = []\n    \n    for lang_pair in LANGUAGE_PAIRS:\n        try:\n            print(f\"Loading {lang_pair} dataset...\")\n            dataset = load_dataset(HF_DATASET_NAME, lang_pair)\n            \n            # Process train split\n            train_mono = convert_to_monolingual(dataset['train'], lang_pair)\n            if TRAIN_SAMPLE_SIZE:\n                train_mono = train_mono.select(range(min(TRAIN_SAMPLE_SIZE, len(train_mono))))\n            train_datasets.append(train_mono)\n            \n            # Process validation split if available\n            if 'validation' in dataset:\n                val_mono = convert_to_monolingual(dataset['validation'], lang_pair)\n                if VAL_SAMPLE_SIZE:\n                    val_mono = val_mono.select(range(min(VAL_SAMPLE_SIZE, len(val_mono))))\n                val_datasets.append(val_mono)\n                \n        except Exception as e:\n            print(f\"Failed to load {lang_pair}: {e}\")\n    \n    # Combine all datasets\n    train_dataset = concatenate_datasets(train_datasets)\n    val_dataset = concatenate_datasets(val_datasets) if val_datasets else None\n    \n    print(f\"Final training dataset size: {len(train_dataset)}\")\n    if val_dataset:\n        print(f\"Final validation dataset size: {len(val_dataset)}\")\n    \n    return train_dataset, val_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:44:12.437982Z","iopub.execute_input":"2025-09-01T04:44:12.438792Z","iopub.status.idle":"2025-09-01T04:44:12.511486Z","shell.execute_reply.started":"2025-09-01T04:44:12.438761Z","shell.execute_reply":"2025-09-01T04:44:12.510795Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def convert_to_monolingual(dataset, lang_pair):\n    \"\"\"Convert parallel corpus to monolingual format\"\"\"\n    src_lang, tgt_lang = lang_pair.split('-')\n    \n    def process_example(example):\n        outputs = []\n        if example['translation'].get(src_lang):\n            outputs.append({\n                \"text\": example['translation'][src_lang].strip(),\n                \"lang\": src_lang\n            })\n        if example['translation'].get(tgt_lang):\n            outputs.append({\n                \"text\": example['translation'][tgt_lang].strip(),\n                \"lang\": tgt_lang\n            })\n        return outputs\n    \n    all_texts = []\n    for example in tqdm(dataset, desc=f\"Processing {lang_pair}\"):\n        all_texts.extend(process_example(example))\n    \n    return Dataset.from_list(all_texts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:44:12.512267Z","iopub.execute_input":"2025-09-01T04:44:12.512588Z","iopub.status.idle":"2025-09-01T04:44:12.530277Z","shell.execute_reply.started":"2025-09-01T04:44:12.512565Z","shell.execute_reply":"2025-09-01T04:44:12.529668Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class T5SpanCorruptionCollator:\n    def __init__(self, tokenizer, noise_density=0.15, mean_noise_span_length=3.0,\n                 input_length=256, target_length=128):\n        self.tokenizer = tokenizer\n        self.noise_density = noise_density\n        self.mean_noise_span_length = mean_noise_span_length\n        self.input_length = input_length\n        self.target_length = target_length\n        self.pad_token_id = tokenizer.pad_token_id\n        self.sentinel_start_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n        \n    def __call__(self, examples):\n        # Get input_ids from tokenized examples\n        input_ids_list = [ex[\"input_ids\"] for ex in examples]\n        \n        corrupted_batch = []\n        labels_batch = []\n        \n        for input_ids in input_ids_list:\n            corrupted, labels = self.corrupt_spans(input_ids)\n            corrupted_batch.append(corrupted)\n            labels_batch.append(labels)\n        \n        # Pad sequences\n        corrupted_batch = self.tokenizer.pad(\n            {\"input_ids\": corrupted_batch},\n            padding=True,\n            max_length=self.input_length,\n            return_tensors=\"pt\",\n        )[\"input_ids\"]\n        \n        labels_batch = self.tokenizer.pad(\n            {\"input_ids\": labels_batch},\n            padding=True,\n            max_length=self.target_length,\n            return_tensors=\"pt\",\n        )[\"input_ids\"]\n        \n        # Replace padding token id with -100 for labels\n        labels_batch[labels_batch == self.tokenizer.pad_token_id] = -100\n        \n        return {\n            \"input_ids\": corrupted_batch,\n            \"labels\": labels_batch,\n            \"attention_mask\": (corrupted_batch != self.tokenizer.pad_token_id).long(),\n        }\n    \n    def corrupt_spans(self, input_ids):\n        \"\"\"Apply span corruption to a sequence\"\"\"\n        # Remove padding tokens\n        input_ids = [id for id in input_ids if id != self.pad_token_id]\n        num_tokens = len(input_ids)\n        \n        # Calculate number of tokens to mask\n        num_noise_tokens = int(round(num_tokens * self.noise_density))\n        if num_noise_tokens == 0:\n            num_noise_tokens = 1  # Ensure at least one token is masked\n        \n        # Create mask (0=keep, 1=mask)\n        mask = np.zeros(num_tokens, dtype=int)\n        mask_indices = np.random.choice(\n            num_tokens, size=num_noise_tokens, replace=False\n        )\n        mask[mask_indices] = 1\n        \n        # Group consecutive masked tokens into spans\n        spans = []\n        i = 0\n        while i < num_tokens:\n            if mask[i] == 0:\n                i += 1\n                continue\n            j = i\n            while j < num_tokens and mask[j] == 1:\n                j += 1\n            spans.append((i, j))\n            i = j\n        \n        # Build corrupted sequence and labels\n        corrupted = []\n        labels = []\n        sentinel_id = self.sentinel_start_id\n        prev = 0\n        \n        for start, end in spans:\n            # Add non-masked tokens\n            corrupted.extend(input_ids[prev:start])\n            # Add sentinel token\n            corrupted.append(sentinel_id)\n            # Add masked tokens to labels with sentinel\n            labels.append(sentinel_id)\n            labels.extend(input_ids[start:end])\n            # Update sentinel ID and prev pointer\n            sentinel_id += 1\n            prev = end\n        \n        # Add remaining tokens\n        corrupted.extend(input_ids[prev:])\n        \n        # Truncate if needed\n        corrupted = corrupted[:self.input_length]\n        labels = labels[:self.target_length]\n        \n        return corrupted, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:44:12.530998Z","iopub.execute_input":"2025-09-01T04:44:12.531215Z","iopub.status.idle":"2025-09-01T04:44:12.553832Z","shell.execute_reply.started":"2025-09-01T04:44:12.531192Z","shell.execute_reply":"2025-09-01T04:44:12.553191Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def evaluate_model(model, tokenizer, eval_dataset, data_collator, num_examples=5):\n    \"\"\"Evaluate the model on validation set and compare with original\"\"\"\n    print(\"Evaluating model...\")\n    \n    # Load metrics\n    bleu_metric = evaluate.load(\"bleu\")\n    rouge_metric = evaluate.load(\"rouge\")\n    \n    # Create text generation pipeline\n    text2text = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n    \n    # Select random examples\n    eval_examples = eval_dataset.shuffle().select(range(min(num_examples, len(eval_dataset))))\n    \n    results = []\n    for example in eval_examples:\n        # Create corrupted input\n        corrupted = data_collator.corrupt_spans(tokenizer.encode(example[\"text\"]))[0]\n        corrupted_text = tokenizer.decode(corrupted, skip_special_tokens=False)\n        \n        # Generate reconstruction\n        output = text2text(corrupted_text, max_length=MAX_TARGET_LENGTH, num_beams=1)\n        reconstructed = output[0][\"generated_text\"]\n        \n        results.append({\n            \"original\": example[\"text\"],\n            \"corrupted\": corrupted_text,\n            \"reconstructed\": reconstructed\n        })\n    \n    # Calculate metrics\n    references = [res[\"original\"] for res in results]\n    predictions = [res[\"reconstructed\"] for res in results]\n    \n    bleu_score = bleu_metric.compute(predictions=predictions, references=references)\n    rouge_score = rouge_metric.compute(predictions=predictions, references=references)\n    \n    print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n    print(f\"ROUGE-L Score: {rouge_score['rougeL']:.4f}\")\n    \n    # Print examples\n    for i, res in enumerate(results[:3]):\n        print(f\"\\nExample {i+1}:\")\n        print(f\"Original: {res['original']}\")\n        print(f\"Corrupted: {res['corrupted']}\")\n        print(f\"Reconstructed: {res['reconstructed']}\")\n    \n    return bleu_score, rouge_score, results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:44:12.554674Z","iopub.execute_input":"2025-09-01T04:44:12.555083Z","iopub.status.idle":"2025-09-01T04:44:12.573428Z","shell.execute_reply.started":"2025-09-01T04:44:12.555054Z","shell.execute_reply":"2025-09-01T04:44:12.572613Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:44:12.574421Z","iopub.execute_input":"2025-09-01T04:44:12.574799Z","iopub.status.idle":"2025-09-01T04:44:45.105116Z","shell.execute_reply.started":"2025-09-01T04:44:12.574771Z","shell.execute_reply":"2025-09-01T04:44:45.104268Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc21dd63070e4e25ab1bdb043a6361b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed4b8894fe7842b0816e1686b001e36d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb3e1661e19413db96ebcca9fae6e8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ff73706ef94625a110961425e096b1"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a17379a927045c5b8034175222e1f76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5535a194984f0dbd1111f5ac4423b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39c859166de4e8f867c8c0b81db7b41"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"train_dataset, val_dataset = prepare_datasets()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:44:45.107789Z","iopub.execute_input":"2025-09-01T04:44:45.108093Z","iopub.status.idle":"2025-09-01T04:45:18.663500Z","shell.execute_reply.started":"2025-09-01T04:44:45.108063Z","shell.execute_reply":"2025-09-01T04:45:18.662376Z"}},"outputs":[{"name":"stdout","text":"Loading en-si dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cffb756e2c54dd28b9ef0ad93cdd070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/155k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a772135c30df42c98675eaedd1891317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/65.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"019b5e0766c8493eb62eb66d215935dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/153k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa82967f228f4caba14e31cec59ccaf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b36b1e25d974b35b51eed8546825239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/979109 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c67304fcf0e342a99eaff88cbacbb759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea26ad7ee0a044cd9158d9d128485147"}},"metadata":{}},{"name":"stderr","text":"Processing en-si: 100%|██████████| 979109/979109 [00:26<00:00, 36632.27it/s]\nProcessing en-si: 100%|██████████| 2000/2000 [00:00<00:00, 40736.43it/s]","output_type":"stream"},{"name":"stdout","text":"Final training dataset size: 1000\nFinal validation dataset size: 250\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        padding=False,  # We'll handle padding in the collator\n        truncation=True,\n        max_length=MAX_INPUT_LENGTH,\n        return_attention_mask=False,\n    )\n\ntokenized_train = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"lang\"]  # Keep \"text\" for debugging if needed\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:45:18.664601Z","iopub.execute_input":"2025-09-01T04:45:18.664783Z","iopub.status.idle":"2025-09-01T04:45:18.754499Z","shell.execute_reply.started":"2025-09-01T04:45:18.664768Z","shell.execute_reply":"2025-09-01T04:45:18.753270Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d9e87d71af4df3b06e312af945a56e"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"print(\"Dataset features:\", tokenized_train.features)\nprint(\"First example:\", tokenized_train[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:45:18.755477Z","iopub.execute_input":"2025-09-01T04:45:18.756015Z","iopub.status.idle":"2025-09-01T04:45:18.761053Z","shell.execute_reply.started":"2025-09-01T04:45:18.755995Z","shell.execute_reply":"2025-09-01T04:45:18.760375Z"}},"outputs":[{"name":"stdout","text":"Dataset features: {'text': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}\nFirst example: {'text': 'Boone!', 'input_ids': [44509, 405, 309, 1]}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"if val_dataset:\n    print(\"comes to this\")\n    tokenized_val = val_dataset.map(\n        tokenize_function,\n        batched=True,\n        remove_columns=val_dataset.column_names,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:45:18.761876Z","iopub.execute_input":"2025-09-01T04:45:18.762125Z","iopub.status.idle":"2025-09-01T04:45:18.833153Z","shell.execute_reply.started":"2025-09-01T04:45:18.762102Z","shell.execute_reply":"2025-09-01T04:45:18.832419Z"}},"outputs":[{"name":"stdout","text":"comes to this\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d39361ff0c6427c9476d0a3ba606cd9"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"data_collator = T5SpanCorruptionCollator(\n        tokenizer,\n        NOISE_DENSITY,\n        MEAN_NOISE_SPAN_LENGTH,\n        MAX_INPUT_LENGTH,\n        MAX_TARGET_LENGTH\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:45:18.833982Z","iopub.execute_input":"2025-09-01T04:45:18.834388Z","iopub.status.idle":"2025-09-01T04:45:18.838038Z","shell.execute_reply.started":"2025-09-01T04:45:18.834370Z","shell.execute_reply":"2025-09-01T04:45:18.837482Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=4,\n    learning_rate=1e-4,\n    warmup_ratio=0.01,\n    num_train_epochs=3,\n    logging_steps=10,\n    save_steps=10,\n    eval_steps=10,\n    save_total_limit=3,\n    predict_with_generate=True,  # Important for seq2seq models\n    remove_unused_columns=False,\n    report_to=\"none\",\n    eval_strategy=\"steps\" if val_dataset else \"no\",  # Changed from evaluation_strategy to eval_strategy\n    load_best_model_at_end=True if val_dataset else False,\n    metric_for_best_model=\"eval_loss\",  \n    greater_is_better=False,  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:45:18.838756Z","iopub.execute_input":"2025-09-01T04:45:18.839099Z","iopub.status.idle":"2025-09-01T04:45:18.881034Z","shell.execute_reply.started":"2025-09-01T04:45:18.839071Z","shell.execute_reply":"2025-09-01T04:45:18.880460Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Initialize Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val if val_dataset else None,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:45:18.881968Z","iopub.execute_input":"2025-09-01T04:45:18.882203Z","iopub.status.idle":"2025-09-01T04:45:19.907564Z","shell.execute_reply.started":"2025-09-01T04:45:18.882187Z","shell.execute_reply":"2025-09-01T04:45:19.906949Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1700408695.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Train model\nprint(\"Starting training..........\")\ntrain_result = trainer.train()\ntrainer.save_model()\ntokenizer.save_pretrained(OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T04:45:19.908268Z","iopub.execute_input":"2025-09-01T04:45:19.908579Z","iopub.status.idle":"2025-09-01T04:46:35.715884Z","shell.execute_reply.started":"2025-09-01T04:45:19.908547Z","shell.execute_reply":"2025-09-01T04:46:35.714533Z"}},"outputs":[{"name":"stderr","text":"You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"Starting training..........\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='31' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [31/96 00:51 < 01:55, 0.56 it/s, Epoch 0.96/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>28.951700</td>\n      <td>22.961605</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>27.743700</td>\n      <td>21.929482</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>25.351600</td>\n      <td>24.853420</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:815] . PytorchStreamWriter failed writing file data/839: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1000239177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training..........\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2620\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m                         self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2623\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m                             \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_only_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m             \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_optimizer_and_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m             \u001b[0;31m# Save RNG state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3335\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3337\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save SCHEDULER & SCALER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:626] . unexpected pos 3872159424 vs 3872159312"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:626] . unexpected pos 3872159424 vs 3872159312","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}